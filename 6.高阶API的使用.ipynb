{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    inputs = tf.placeholder(shape=[None,784],dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None,10],dtype=tf.float32)\n",
    "    \n",
    "    hidden_weight = tf.Variable(tf.random_normal([784,128]),name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128,]),name='hidden_bias')\n",
    "    \n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs,hidden_weight) + hidden_bias)\n",
    "    \n",
    "    output_weight = tf.Variable(tf.random_normal([128,10]),name='output_weight')\n",
    "    output_bias = tf.Variable(tf.zeros([10,]),name='output_bias')\n",
    "    \n",
    "    logits = tf.matmul(hidden_output,output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=labels))\n",
    "    \n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(labels,axis=1),tf.argmax(output,axis=1)),tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 114.1995, acc 0.2024\n",
      "step   500, loss 3.0882, acc 0.7631\n",
      "step  1000, loss 1.0618, acc 0.8075\n",
      "step  1500, loss 6.2814, acc 0.8305\n",
      "step  2000, loss 3.0803, acc 0.8397\n",
      "step  2500, loss 4.5529, acc 0.8536\n",
      "step  3000, loss 0.1493, acc 0.8555\n",
      "step  3500, loss 5.4211, acc 0.8659\n",
      "step  4000, loss 2.9003, acc 0.8653\n",
      "step  4500, loss 2.1427, acc 0.8733\n",
      "step  5000, loss 1.4414, acc 0.8719\n",
      "step  5500, loss 1.6811, acc 0.8734\n",
      "step  6000, loss 1.1963, acc 0.8767\n",
      "step  6500, loss 1.5536, acc 0.8770\n",
      "step  7000, loss 0.8804, acc 0.8803\n",
      "step  7500, loss 1.6436, acc 0.8803\n",
      "step  8000, loss 0.9773, acc 0.8837\n",
      "step  8500, loss 0.8565, acc 0.8857\n",
      "step  9000, loss 3.4974, acc 0.8837\n",
      "step  9500, loss 0.6003, acc 0.8891\n",
      "step 10000, loss 0.3092, acc 0.8871\n",
      "step 10500, loss 0.1238, acc 0.8903\n",
      "step 11000, loss 0.2798, acc 0.8904\n",
      "step 11500, loss 0.3829, acc 0.8875\n",
      "step 12000, loss 1.8498, acc 0.8900\n",
      "step 12500, loss 2.0709, acc 0.8862\n",
      "step 13000, loss 2.2715, acc 0.8883\n",
      "step 13500, loss 0.7257, acc 0.8914\n",
      "step 14000, loss 1.8524, acc 0.8943\n",
      "step 14500, loss 1.4550, acc 0.8922\n",
      "step 15000, loss 1.4704, acc 0.8971\n",
      "step 15500, loss 0.1365, acc 0.8941\n",
      "step 16000, loss 0.6658, acc 0.8955\n",
      "step 16500, loss 1.4681, acc 0.8943\n",
      "step 17000, loss 0.4934, acc 0.9008\n",
      "step 17500, loss 0.0813, acc 0.8953\n",
      "step 18000, loss 0.5138, acc 0.8946\n",
      "step 18500, loss 1.2289, acc 0.8975\n",
      "step 19000, loss 0.0142, acc 0.8988\n",
      "step 19500, loss 0.6721, acc 0.9008\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss,_ = sess.run([loss,train_op],feed_dict={\n",
    "            inputs:batch_images,\n",
    "            labels:batch_labels\n",
    "        })\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images,batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc,feed_dict={\n",
    "                    inputs:batch_images,\n",
    "                    labels:batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用高阶API构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g1:\n",
    "    inputs = tf.placeholder(shape=[None,784],dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None,10],dtype=tf.float32)\n",
    "    \n",
    "    hidden_output = tf.keras.layers.Dense(128,activation=tf.nn.relu)(inputs)\n",
    "    logits = tf.keras.layers.Dense(10,activation=None)(hidden_output)\n",
    "    \n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits,labels=labels))\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(labels,axis=1),tf.argmax(output,axis=1)),\n",
    "                                tf.float32))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 2.3885, acc 0.0815\n",
      "step   500, loss 0.6244, acc 0.8549\n",
      "step  1000, loss 0.6903, acc 0.8848\n",
      "step  1500, loss 0.4401, acc 0.8974\n",
      "step  2000, loss 0.6157, acc 0.9075\n",
      "step  2500, loss 0.2208, acc 0.9088\n",
      "step  3000, loss 0.2388, acc 0.9141\n",
      "step  3500, loss 0.4034, acc 0.9185\n",
      "step  4000, loss 0.3893, acc 0.9219\n",
      "step  4500, loss 0.2438, acc 0.9233\n",
      "step  5000, loss 0.1616, acc 0.9252\n",
      "step  5500, loss 0.2773, acc 0.9283\n",
      "step  6000, loss 0.1970, acc 0.9302\n",
      "step  6500, loss 0.0639, acc 0.9298\n",
      "step  7000, loss 0.3346, acc 0.9342\n",
      "step  7500, loss 0.3655, acc 0.9341\n",
      "step  8000, loss 0.1904, acc 0.9348\n",
      "step  8500, loss 0.1393, acc 0.9350\n",
      "step  9000, loss 0.1413, acc 0.9370\n",
      "step  9500, loss 0.3052, acc 0.9383\n",
      "step 10000, loss 0.1346, acc 0.9407\n",
      "step 10500, loss 0.5041, acc 0.9403\n",
      "step 11000, loss 0.0985, acc 0.9437\n",
      "step 11500, loss 0.1787, acc 0.9432\n",
      "step 12000, loss 0.2274, acc 0.9424\n",
      "step 12500, loss 0.2433, acc 0.9473\n",
      "step 13000, loss 0.2484, acc 0.9439\n",
      "step 13500, loss 0.1602, acc 0.9465\n",
      "step 14000, loss 0.2558, acc 0.9467\n",
      "step 14500, loss 0.1490, acc 0.9495\n",
      "step 15000, loss 0.1923, acc 0.9494\n",
      "step 15500, loss 0.0367, acc 0.9492\n",
      "step 16000, loss 0.0566, acc 0.9492\n",
      "step 16500, loss 0.5033, acc 0.9525\n",
      "step 17000, loss 0.1395, acc 0.9517\n",
      "step 17500, loss 0.1613, acc 0.9522\n",
      "step 18000, loss 0.2101, acc 0.9513\n",
      "step 18500, loss 0.1483, acc 0.9539\n",
      "step 19000, loss 0.1734, acc 0.9529\n",
      "step 19500, loss 0.1442, acc 0.9567\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g1) as sess:\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用高阶API使得模型的效率变高了，代码简洁方便"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
