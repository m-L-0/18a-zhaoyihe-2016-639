1.导入iris数据集：

```python
from sklearn.datasets import load_iris
iris = load_iris()
```

2.求数据集的相似度矩阵：

​	这里分别试用了两种求相似度的方法：余弦相似度和欧式距离相似度。

```python
#余弦相似度
from sklearn.metrics.pairwise import cosine_similarity
sim = cosine_similarity(x)+1 #使其相似值都大于0
#欧式距离相似度
import numpy.linalg as lianlg
d[i,j] = np.linalg.norm(x[i]-x[j])  
```

​	所用余弦相似度，使其后面所求正确率高。但其邻接矩阵的分布情况不好

​	所用欧式距离相似度，其邻接矩阵的分布情况也仅仅是第一类的分布情况优。

​	在这里采用余弦相似度（为了高准确率）

3.求度数矩阵 d

4.标准化的拉普拉斯矩阵：

```Python
L = d^(1/2) *L *d^(1/2)
```

5.求L的前k个最小值的特征向量

```python
import numpy.linalg as lianlg
#求特征值及特征向量
eig,feature=np.linalg.eig(L)
#前k个最小特征向量所对应的样本索引
indices = np.argsort(eig)[:k]
```

6.k-means的实现：

  (1).随机获取k个质心点

```python
def Center(data,k):
    indexs = np.random.random_integers(0,len(data)-1,k)
    centers = []
    for index in indexs:
        centers.append(data[index])
    return centers
```

  (2).求欧式距离

```python
def Distance(vec1,vec2):
    dist = np.sqrt(np.sum(np.square(vec1 - vec2)))
    #dist = numpy.linalg.norm(vec1 - vec2)  
    return dist
```

  (3).第i个数据点到第j个中心点更近，则将i划分到j中;更新质心点

```python
#k-means聚类
def kmeans(data, k, Distance = Distance, Center = Center):
    num = data.shape[0]
    #存放该样本属于哪类，及质心距离
    clusterAssment = np.mat(np.zeros((num, 2)))
    centroids = Center(data,k)
    centroids = np.array(centroids)   
    #用于判断是否已经收敛
    clusterChanged = True
    while clusterChanged:
        clusterChanged = False
        for i in range(num):
            minDist  = 100000
            minIndex = -1 
            for j in range(k):
                distJI = Distance(centroids[j,:], data[i,:])
                if distJI < minDist:
                    minDist  = distJI
                     #如果第i个数据点到第j个中心点更近，则将i划分到j中
                    minIndex = j 
                    
            if clusterAssment[i, 0] != minIndex:
                    clusterChanged = True
                    #将第i个数据点的分配情况存入字典
                    clusterAssment[i, :] = minIndex, minDist**2 
        #更新中心点
        for center in range(k):
            #.A将matrix转化为array，nonzero返回=center的下标
            ptsInClust = data[np.nonzero(
                clusterAssment[:,0].A == center)[0]] 
            centroids[center,:] = np.mean(ptsInClust, axis = 0)
    return centroids, clusterAssment        
```

7.利用所求的特征向量，将数据集变为150*k的样本数据。利用k-means进行聚类

```python
for i in range(20):
    centroids,clustAssment = kmeans(ksim,3)
    a=clustAssment.A[:,0]
    print(centroids)
    print(accuracy_score(y,a))
```

找到最佳的聚类中心点

```python
 centroids = np.array([[ 1.,          0.,          0.,        ],
                       [ 0.,         -0.68347619,  0.69753973],
                       [ 0.,         -0.74373286, -0.61969235]])
```

8.画图

```Python
node_color=[]
node_color0=[]
node_color1=[]
node_color2=[]
for i in range(150):
    if(a[i]==0):
        color = 'r'
        node_color0.append(color)
    elif(a[i]==1):
        color = 'g'
        node_color1.append(color)
    else:
        color = 'b'
        node_color1.append(color)
    node_color.append(color)
H = nx.Graph()
for i in range(0,150):
    for j in range(i,150):
        if matrix[i][j]>1.9991:
            H.add_edge(i,j)
nx.draw(H,node_color=node_color,node_size=100)
plt.show() 
```

